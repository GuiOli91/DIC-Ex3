{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "780b240e-4e7c-4b0d-836c-d4dde2b6cf31",
   "metadata": {},
   "source": [
    "## Part 3) Datasets/DataFrames: Spark ML and Pipelines\n",
    "\n",
    "This is the Jupyter Notebook for Part 3 of exercise 2.\n",
    "\n",
    "In the first code cell we simply import necessary libraries and create a SparkSession object with application name 'Ex2_Part3' using SparkSession from the pyspark library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d10981-205a-4f7d-9ebe-e8441f319483",
   "metadata": {},
   "source": [
    "In the following cell the code is identical to part2 of the exercise. For explanation have a look at the jupyter notebook of part2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6731103-af87-4f28-a415-413551f6cbd6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "23/05/18 15:25:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/18 15:25:11 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/05/18 15:25:11 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/05/18 15:25:11 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/05/18 15:25:11 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "23/05/18 15:25:11 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "23/05/18 15:25:11 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "23/05/18 15:25:11 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "23/05/18 15:25:11 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "23/05/18 15:25:11 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.\n",
      "23/05/18 15:25:11 WARN Utils: Service 'SparkUI' could not bind on port 4049. Attempting port 4050.\n",
      "23/05/18 15:25:11 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, ChiSqSelector, RegexTokenizer, CountVectorizer, StringIndexer, Normalizer\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Ex2_Part3\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be1b11c9-0605-48c2-a3fb-1f7247b33862",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "input_file = \"hdfs:///user/dic23_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "data = spark.read.json(input_file)\n",
    "\n",
    "notebook_path = os.path.abspath(\"\")\n",
    "\n",
    "# Load the stopwords into a list\n",
    "stopwords_file = open(f\"{notebook_path}/stopwords.txt\", \"r\")\n",
    "words_data = stopwords_file.read()\n",
    "stopwords = words_data.split(\"\\n\")\n",
    "stopwords_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee580d9d-52d5-4240-9a13-02b6580f35de",
   "metadata": {},
   "source": [
    "In the following cell the data is split into training and test sets, with 80% of the data going to the training set and 20% to the test set. A seed is used for reproducible random splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "020cb931-2178-47dc-8ec1-85e19ec21455",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "training_data, test_data = data.randomSplit([0.8, 0.2], seed=1205)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc9c1d0-7d03-48bb-aaad-3ed4fa9d317c",
   "metadata": {
    "tags": []
   },
   "source": [
    "In the following two cells the code is identical to part2 of the exercise. For explanation have a look at the jupyter notebook of part2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63e6e87d-ffc1-4468-80fd-e79c6d9794e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build the transformation pipeline\n",
    "tokenizer = RegexTokenizer(pattern=r'[\\s\\d\\(\\)\\[\\]\\{\\}\\.,!?\\-,;:+\\=_\"\\'`~#@&*%€$§\\\\/]+',\n",
    "                           inputCol=\"reviewText\",\n",
    "                           outputCol=\"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c2a320a-86dd-4e50-9198-2afa1c610d3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_words_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_words\", stopWords=stopwords)\n",
    "count_vectorizer = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"raw_features\", minDF=5)\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\", minDocFreq=5)\n",
    "labelIndexer = StringIndexer(inputCol=\"category\", outputCol=\"label\")\n",
    "chi_sq_selector = ChiSqSelector(numTopFeatures=2000, featuresCol=\"features\", outputCol=\"selected_features\", labelCol=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6100de70-d839-4819-854c-7a319d387f89",
   "metadata": {},
   "source": [
    "### New to part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501215bd-7fe2-4f27-b4a1-93d2472545ae",
   "metadata": {},
   "source": [
    "This section of the pipeline introduces a normalized Support Vector Machine (SVM) classifier within a OneVsRest strategy for multi-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a1ae7a-91c2-42a9-b42d-1f349a6acff7",
   "metadata": {},
   "source": [
    "The Normalizer streamlines each data sample in the dataset to have a unit norm. It employs L2 normalization according to the p=2.0 parameter.\n",
    "\n",
    "The Normalizer processes the \"selected_features\" obtained from the ChiSqSelector output, generating \"normalized_features\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa700878-b89c-4b85-843d-c5887cfce29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer(inputCol=\"selected_features\", outputCol=\"normalized_features\", p=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c77962-aa1d-464f-b842-ed60bae03451",
   "metadata": {},
   "source": [
    "For the classification task we use a SVM classifier. It tries to find the best hyperplane to separate different classes. In this case, we use the normalized features as input to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d60d3364-7b4a-4a74-b6a1-03cfe72144c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC(featuresCol=\"normalized_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd2567d-f82f-4234-9d42-06a5a1b4488d",
   "metadata": {},
   "source": [
    "In the next chunk of code we define OneVsRest. It is a strategy for multi-class classification. It involves training a single classifier per class, with the samples of that class as positive samples and all other samples as negatives. In this case, the binary classifier used is the SVM classifier defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87a4d77e-be32-47c9-8336-fa6d855938de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ovr = OneVsRest(classifier=svm, labelCol=\"label\", featuresCol=\"normalized_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d428193b-8d09-4ae1-8ef3-550ce5f2cc4a",
   "metadata": {},
   "source": [
    "Now we define the whole pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8aaf6c78-22c0-4abb-9e71-ddaaad691edd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer, stop_words_remover, count_vectorizer, idf, labelIndexer, chi_sq_selector, normalizer, ovr]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fae306-7318-4d46-b642-0a32be27cd91",
   "metadata": {},
   "source": [
    "In the next piece of code we define a grid to fine-tune our machine learning model using Spark's 'ParamGridBuilder()'. \n",
    "Parameters to be tuned include the number of top features selected (`numTopFeatures`), the regularization parameter (`regParam`), maximum iterations (`maxIter`), and whether to standardize training features (`standardization`).\n",
    "We defined several values as given in the exercise. \n",
    "The 'build()' function is used to create all possible mixtures of these parameter values, ready for the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac35347b-3b77-43d8-84e8-bfbc2697b461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the parameter grid for grid search\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(chi_sq_selector.numTopFeatures, [400, 2000]) \\\n",
    "    .addGrid(ovr.getClassifier().regParam, [0.01, 0.1, 0.4]) \\\n",
    "    .addGrid(ovr.getClassifier().maxIter, [7, 14]) \\\n",
    "    .addGrid(ovr.getClassifier().standardization, [False, True]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258552f4-41f9-4032-8327-74838b8b5709",
   "metadata": {},
   "source": [
    "In the next piece of code we're initializing a MulticlassClassificationEvaluator. The evaluator will use the F1 score to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81a6ea0b-dce5-4e9b-9e74-ef1862a20764",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"f1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb3e675-f9bd-47c6-ae48-cb760fd0bb07",
   "metadata": {},
   "source": [
    "This block of code sets up a Train-Validation Split model selection method. It takes as input the previously defined pipeline estimator, the parameter grid for tuning, and the evaluator for model assessment. The `trainRatio` parameter is set to 0.8, indicating that 80% of the data will be used for training, while the remaining 20% is for validation. The `seed` parameter is set for reproducibility, ensuring that the same split occurs every time the code is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07d47221-3a59-4f55-b83d-75660efcb7d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the train-validation split\n",
    "tvs = TrainValidationSplit(estimator=pipeline, \n",
    "                           estimatorParamMaps=paramGrid, \n",
    "                           evaluator=evaluator, \n",
    "                           trainRatio=0.8, \n",
    "                           seed=1205)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d127e3-5c1c-4518-8480-12e22b0b38e4",
   "metadata": {},
   "source": [
    "Now we train our model using the defined train-validation split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64518ed5-53a8-4dd4-bec9-dc0d230a0c4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/18 15:25:52 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/05/18 15:25:52 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "23/05/18 15:25:53 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/05/18 15:25:53 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "23/05/18 16:12:26 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n",
      "23/05/18 16:12:43 WARN BlockManager: Asked to remove block broadcast_26562, which does not exist\n",
      "23/05/18 16:12:43 WARN BlockManager: Asked to remove block broadcast_26562_piece0, which does not exist\n",
      "23/05/18 16:13:08 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n",
      "23/05/18 16:13:40 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n",
      "23/05/18 16:13:55 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n",
      "23/05/18 16:24:11 WARN DAGScheduler: Broadcasting large task binary with size 1220.2 KiB\n",
      "23/05/18 16:27:50 WARN DAGScheduler: Broadcasting large task binary with size 1220.2 KiB\n",
      "23/05/18 16:32:40 WARN DAGScheduler: Broadcasting large task binary with size 1220.2 KiB\n",
      "23/05/18 16:37:01 WARN DAGScheduler: Broadcasting large task binary with size 1220.2 KiB\n",
      "23/05/18 16:40:50 WARN DAGScheduler: Broadcasting large task binary with size 1220.2 KiB\n",
      "23/05/18 16:44:27 WARN DAGScheduler: Broadcasting large task binary with size 1220.2 KiB\n",
      "23/05/18 16:50:02 WARN DAGScheduler: Broadcasting large task binary with size 1220.2 KiB\n",
      "23/05/18 16:54:22 WARN DAGScheduler: Broadcasting large task binary with size 1220.2 KiB\n",
      "23/05/18 16:55:55 WARN BlockManager: Asked to remove block broadcast_49466, which does not exist\n",
      "23/05/18 16:58:15 WARN DAGScheduler: Broadcasting large task binary with size 1220.2 KiB\n",
      "23/05/18 17:01:49 WARN DAGScheduler: Broadcasting large task binary with size 1220.2 KiB\n",
      "23/05/18 17:07:35 WARN DAGScheduler: Broadcasting large task binary with size 1220.2 KiB\n",
      "23/05/18 17:11:43 WARN DAGScheduler: Broadcasting large task binary with size 1220.2 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 6664.386291503906 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train the model using the train-validation split\n",
    "start_time = time.time()\n",
    "model = tvs.fit(training_data)\n",
    "end_time = time.time()\n",
    "print(\"Training time: {} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53347048-b3e7-41f3-bfef-43d3983fd045",
   "metadata": {},
   "source": [
    "Now we identify and analyze the top-performing model from our training. \n",
    "\n",
    "First, we retrieve the hyperparameters associated with the best model. Then we apply this model to the test data to generate predictions. Lastly, we compute and print out the F1 score, precision, and recall based on these predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f640e863-dede-4921-b1c2-351a6a33306c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters of the best model:\n",
      "numTopFeatures: 2000\n",
      "regParam: 0.01\n",
      "maxIter: 14\n",
      "standardization: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/18 17:16:34 WARN DAGScheduler: Broadcasting large task binary with size 1283.4 KiB\n",
      "23/05/18 17:17:10 WARN DAGScheduler: Broadcasting large task binary with size 1283.4 KiB\n",
      "23/05/18 17:17:21 WARN DAGScheduler: Broadcasting large task binary with size 1283.4 KiB\n",
      "[Stage 40722:=================================================>   (13 + 1) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics of the best model:\n",
      "F1 score: 0.6069700943577272\n",
      "Precision: 0.5923226672490524\n",
      "Recall: 0.6369466800804827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get the best model\n",
    "best_model = model.bestModel\n",
    "\n",
    "# Get the stages of the best model\n",
    "best_model_stages = best_model.stages\n",
    "\n",
    "# Get the list of parameter maps\n",
    "param_maps = tvs.getEstimatorParamMaps()\n",
    "\n",
    "# Get the validation metrics\n",
    "validation_metrics = model.validationMetrics\n",
    "\n",
    "# Get the index of the best model\n",
    "best_model_index = validation_metrics.index(max(validation_metrics))\n",
    "\n",
    "# Get the parameter map of the best model\n",
    "best_model_params = param_maps[best_model_index]\n",
    "\n",
    "# Print the hyperparameters of the best model\n",
    "print(\"Hyperparameters of the best model:\")\n",
    "for param in best_model_params:\n",
    "    print(f\"{param.name}: {best_model_params[param]}\")\n",
    "\n",
    "# Get the validation metric of the best model\n",
    "best_model_metric = validation_metrics[best_model_index]\n",
    "\n",
    "# Instantiate evaluators for the different metrics\n",
    "f1_evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"f1\")\n",
    "precision_evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"weightedPrecision\")\n",
    "recall_evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"weightedRecall\")\n",
    "\n",
    "# Make predictions on the validation data\n",
    "predictions = best_model.transform(test_data)\n",
    "\n",
    "# Get the metrics of the best model\n",
    "best_model_f1 = f1_evaluator.evaluate(predictions)\n",
    "best_model_precision = precision_evaluator.evaluate(predictions)\n",
    "best_model_recall = recall_evaluator.evaluate(predictions)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"\\nMetrics of the best model:\")\n",
    "print(f\"F1 score: {best_model_f1}\")\n",
    "print(f\"Precision: {best_model_precision}\")\n",
    "print(f\"Recall: {best_model_recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3545da-2ddf-40a9-9886-7f80548ce03c",
   "metadata": {},
   "source": [
    "Lastly we print the performance of all the models we trained with the train validation split, to get an idea how important which hyperparameters are. \n",
    "The snippet retrieves the list of hyperparameters used during model training and their corresponding validation metrics. It then loops through this list, printing out the F1 score and specific hyperparameters for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69c0d746-aa9c-4829-ba59-20b56e9a10b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 metrics:\n",
      "Validation metric (F1 score): 0.05181221746653008\n",
      "Hyperparameters:\n",
      "  numTopFeatures: 400\n",
      "  regParam: 0.01\n",
      "  maxIter: 7\n",
      "  standardization: False\n",
      "\n",
      "\n",
      "Model 2 metrics:\n",
      "Validation metric (F1 score): 0.4546486697658312\n",
      "Hyperparameters:\n",
      "  numTopFeatures: 400\n",
      "  regParam: 0.01\n",
      "  maxIter: 7\n",
      "  standardization: True\n",
      "\n",
      "\n",
      "Model 3 metrics:\n",
      "Validation metric (F1 score): 0.39225529544944576\n",
      "Hyperparameters:\n",
      "  numTopFeatures: 400\n",
      "  regParam: 0.01\n",
      "  maxIter: 14\n",
      "  standardization: False\n",
      "\n",
      "\n",
      "Model 4 metrics:\n",
      "Validation metric (F1 score): 0.4529093251337653\n",
      "Hyperparameters:\n",
      "  numTopFeatures: 400\n",
      "  regParam: 0.01\n",
      "  maxIter: 14\n",
      "  standardization: True\n",
      "\n",
      "\n",
      "Model 5 metrics:\n",
      "Validation metric (F1 score): 0.24522917274215697\n",
      "Hyperparameters:\n",
      "  numTopFeatures: 400\n",
      "  regParam: 0.1\n",
      "  maxIter: 7\n",
      "  standardization: False\n",
      "\n",
      "\n",
      "Model 6 metrics:\n",
      "Validation metric (F1 score): 0.44388646368741896\n",
      "Hyperparameters:\n",
      "  numTopFeatures: 400\n",
      "  regParam: 0.1\n",
      "  maxIter: 7\n",
      "  standardization: True\n",
      "\n",
      "\n",
      "Model 7 metrics:\n",
      "Validation metric (F1 score): 4.620986581256031e-05\n",
      "Hyperparameters:\n",
      "  numTopFeatures: 400\n",
      "  regParam: 0.1\n",
      "  maxIter: 14\n",
      "  standardization: False\n",
      "\n",
      "\n",
      "Model 8 metrics:\n",
      "Validation metric (F1 score): 0.43947179886128995\n",
      "Hyperparameters:\n",
      "  numTopFeatures: 400\n",
      "  regParam: 0.1\n",
      "  maxIter: 14\n",
      "  standardization: True\n",
      "\n",
      "\n",
      "Model 9 metrics:\n",
      "Validation metric (F1 score): 4.620986581256031e-05\n",
      "Hyperparameters:\n",
      "  numTopFeatures: 400\n",
      "  regParam: 0.4\n",
      "  maxIter: 7\n",
      "  standardization: False\n",
      "\n",
      "\n",
      "Model 10 metrics:\n",
      "Validation metric (F1 score): 0.4169512432769322\n",
      "Hyperparameters:\n",
      "  numTopFeatures: 400\n",
      "  regParam: 0.4\n",
      "  maxIter: 7\n",
      "  standardization: True\n",
      "\n",
      "\n",
      "Model 11 metrics:\n",
      "Validation metric (F1 score): 4.620986581256031e-05\n",
      "Hyperparameters:\n",
      "  numTopFeatures: 400\n",
      "  regParam: 0.4\n",
      "  maxIter: 14\n",
      "  standardization: False\n",
      "\n",
      "\n",
      "Model 12 metrics:\n",
      "Validation metric (F1 score): 0.42122911345749914\n",
      "Hyperparameters:\n",
      "  numTopFeatures: 400\n",
      "  regParam: 0.4\n",
      "  maxIter: 14\n",
      "  standardization: True\n",
      "\n",
      "\n",
      "Model 13 metrics:\n",
      "Validation metric (F1 score): 0.12958317577512538\n",
      "Hyperparameters:\n",
      "  numTopFeatures: 2000\n",
      "  regParam: 0.01\n",
      "  maxIter: 7\n",
      "  standardization: False\n",
      "\n",
      "\n",
      "Model 14 metrics:\n",
      "Validation metric (F1 score): 0.6064453741498723\n",
      "Hyperparameters:\n",
      "  numTopFeatures: 2000\n",
      "  regParam: 0.01\n",
      "  maxIter: 7\n",
      "  standardization: True\n",
      "\n",
      "\n",
      "Model 15 metrics:\n",
      "Validation metric (F1 score): 0.4852844975636569\n",
      "Hyperparameters:\n",
      "  numTopFeatures: 2000\n",
      "  regParam: 0.01\n",
      "  maxIter: 14\n",
      "  standardization: False\n",
      "\n",
      "\n",
      "Model 16 metrics:\n",
      "Validation metric (F1 score): 0.6094119742334246\n",
      "Hyperparameters:\n",
      "  numTopFeatures: 2000\n",
      "  regParam: 0.01\n",
      "  maxIter: 14\n",
      "  standardization: True\n",
      "\n",
      "\n",
      "Model 17 metrics:\n",
      "Validation metric (F1 score): 0.12591595110263898\n",
      "Hyperparameters:\n",
      "  numTopFeatures: 2000\n",
      "  regParam: 0.1\n",
      "  maxIter: 7\n",
      "  standardization: False\n",
      "\n",
      "\n",
      "Model 18 metrics:\n",
      "Validation metric (F1 score): 0.6041606537049403\n",
      "Hyperparameters:\n",
      "  numTopFeatures: 2000\n",
      "  regParam: 0.1\n",
      "  maxIter: 7\n",
      "  standardization: True\n",
      "\n",
      "\n",
      "Model 19 metrics:\n",
      "Validation metric (F1 score): 0.12731353644219492\n",
      "Hyperparameters:\n",
      "  numTopFeatures: 2000\n",
      "  regParam: 0.1\n",
      "  maxIter: 14\n",
      "  standardization: False\n",
      "\n",
      "\n",
      "Model 20 metrics:\n",
      "Validation metric (F1 score): 0.6041443658077502\n",
      "Hyperparameters:\n",
      "  numTopFeatures: 2000\n",
      "  regParam: 0.1\n",
      "  maxIter: 14\n",
      "  standardization: True\n",
      "\n",
      "\n",
      "Model 21 metrics:\n",
      "Validation metric (F1 score): 0.12591595110263898\n",
      "Hyperparameters:\n",
      "  numTopFeatures: 2000\n",
      "  regParam: 0.4\n",
      "  maxIter: 7\n",
      "  standardization: False\n",
      "\n",
      "\n",
      "Model 22 metrics:\n",
      "Validation metric (F1 score): 0.5756562313232148\n",
      "Hyperparameters:\n",
      "  numTopFeatures: 2000\n",
      "  regParam: 0.4\n",
      "  maxIter: 7\n",
      "  standardization: True\n",
      "\n",
      "\n",
      "Model 23 metrics:\n",
      "Validation metric (F1 score): 0.04765305190536646\n",
      "Hyperparameters:\n",
      "  numTopFeatures: 2000\n",
      "  regParam: 0.4\n",
      "  maxIter: 14\n",
      "  standardization: False\n",
      "\n",
      "\n",
      "Model 24 metrics:\n",
      "Validation metric (F1 score): 0.5810729236879203\n",
      "Hyperparameters:\n",
      "  numTopFeatures: 2000\n",
      "  regParam: 0.4\n",
      "  maxIter: 14\n",
      "  standardization: True\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the list of hyperparameters and their corresponding validation metrics\n",
    "param_maps = tvs.getEstimatorParamMaps()\n",
    "validation_metrics = model.validationMetrics\n",
    "\n",
    "# Iterate over the list and print the hyperparameters and metrics\n",
    "for i in range(len(param_maps)):\n",
    "    print(f\"Model {i+1} metrics:\")\n",
    "    print(f\"Validation metric (F1 score): {validation_metrics[i]}\")\n",
    "    print(\"Hyperparameters:\")\n",
    "    for param in param_maps[i]:\n",
    "        print(f\"  {param.name}: {param_maps[i][param]}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7edec6b1-e247-4b32-a3a8-d44cb66e7ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DIC23)",
   "language": "python",
   "name": "python3_dic23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
