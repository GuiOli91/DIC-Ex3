{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70ed954c-beb3-4ea6-961f-cdd254c0039d",
   "metadata": {},
   "source": [
    "## Part 2) Datasets/DataFrames: Spark ML and Pipelines\n",
    "\n",
    "This is the Jupyter Notebook for Part 2 of exercise 2\n",
    "\n",
    "In the first code cell we simply import necessary libraries and create a SparkSession object with application name 'Ex2_Part2' using SparkSession from the pyspark library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edf9c3e4-ec42-4f19-97c5-78f450c7409d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Ex2_Part2\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1628f6ed-669b-41df-8d1b-83c7d323b31a",
   "metadata": {},
   "source": [
    "### Loading the Dataset and stopwords.txt\n",
    "\n",
    "Next we load the input data from the given HDFS path in a DataFrame called df.\n",
    "We also print the schema of the DataFrame as well as show the data in df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d07de58e-e133-4ffa-b30d-ff934d209ddc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- helpful: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      "\n",
      "+----------+--------------------+--------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "|      asin|            category| helpful|overall|          reviewText| reviewTime|    reviewerID|        reviewerName|             summary|unixReviewTime|\n",
      "+----------+--------------------+--------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "|0981850006|Patio_Lawn_and_Garde|  [6, 7]|    5.0|This was a gift f...| 12 3, 2009|A2VNYWOPJ13AFP|Amazon Customer \"...|              Delish|    1259798400|\n",
      "|B00002N66D|Patio_Lawn_and_Garde|  [1, 1]|    5.0|This is a very ni...| 12 3, 2012|A2E5XXXC07AGA7|               James|       Nice spreader|    1354492800|\n",
      "|B00002N67U|Patio_Lawn_and_Garde|  [0, 1]|    1.0|The metal base wi...|08 13, 2008|A16PX63WZIEQ13|             Finaldx| Terrible spike base|    1218585600|\n",
      "|B00002N6AN|Patio_Lawn_and_Garde|  [0, 0]|    4.0|For the most part...| 10 1, 2009|A2OSWM3522VARA|Wayne Allen \"Moto...|   gets the job done|    1254355200|\n",
      "|B00002N8K3|Patio_Lawn_and_Garde|  [4, 5]|    1.0|This hose is supp...|07 13, 2013|A2SX9YPPGEUADI|HappyCamper \"Happ...|           The worst|    1373673600|\n",
      "|B00002NBQ8|Patio_Lawn_and_Garde|  [2, 2]|    5.0|This tool works v...| 04 1, 2014|A2PENG0PDZUEGN|John Grossbohlin ...|          Great tool|    1396310400|\n",
      "|B00004DTNG|Patio_Lawn_and_Garde|  [1, 2]|    4.0|This product is a...|10 31, 2008|A2NBUMLJ0QBTND|            L. Allen|Black & Decker El...|    1225411200|\n",
      "|B00004OCJZ|Patio_Lawn_and_Garde|[10, 12]|    1.0|I was excited to ...|12 15, 2011| AC9EDJLYU6DEH|               NandM|Seems Like Good I...|    1323907200|\n",
      "|B00004R9TJ|Patio_Lawn_and_Garde|  [0, 1]|    5.0|I purchased the L...|11 28, 2009|A2OKI2AQ4QQV8R|          Ian Koenig|  Speeds up mulching|    1259366400|\n",
      "|B00004R9UK|Patio_Lawn_and_Garde|  [0, 0]|    4.0|Never used a manu...| 09 8, 2013|A1DYWRLW6ZB4FW|          guitarhero|  Works as described|    1378598400|\n",
      "|B00004R9VV|Patio_Lawn_and_Garde|  [1, 1]|    5.0|Good price. Good ...| 11 6, 2013|A3D8QKK1Z19XMM|         Craig S Fry|           Satisfied|    1383696000|\n",
      "|B00004R9VV|Patio_Lawn_and_Garde|[13, 13]|    5.0|I have owned the ...|04 12, 2008| A2OAGDTF00OVD|           M. Roldan|Flowtron 15 watt ...|    1207958400|\n",
      "|B00004R9XC|Patio_Lawn_and_Garde|  [2, 2]|    5.0|I had \"won\" a sim...| 09 9, 2009|A1V24A11LXC8FM|          L. Edwards|   Multipurpose ties|    1252454400|\n",
      "|B00004RA0O|Patio_Lawn_and_Garde|  [0, 0]|    4.0|The birds ate all...|07 18, 2013|A1ZFSH5UIXYC5Y|         Music lover|This netting does...|    1374105600|\n",
      "|B00004RA3E|Patio_Lawn_and_Garde|[12, 13]|    5.0|Bought last summe...|03 30, 2005|A1D8C0T4T353M9|Lives up North \"E...|Perfect for right...|    1112140800|\n",
      "|B00004RA4G|Patio_Lawn_and_Garde|  [0, 0]|    4.0|I knew I had a mo...| 06 3, 2014|A33ML4YVHKCTQH|            J. Brody|Be patient--it do...|    1401753600|\n",
      "|B00004RA88|Patio_Lawn_and_Garde|  [0, 0]|    4.0|I was a little wo...|04 10, 2013|A34Y0G9RRXQXZJ|             Greg S.|      Chainsaw Chain|    1365552000|\n",
      "|B00004RA91|Patio_Lawn_and_Garde| [8, 10]|    5.0|I have used this ...| 07 7, 2000|A2FWAQS2ZYELDY|             Lisa H.|  Hummingbird Nectar|     962928000|\n",
      "|B00004RAL1|Patio_Lawn_and_Garde|[10, 10]|    5.0|I actually do not...|02 18, 2001|A3QBS6EPFZZH6V|        Nancy Phipps|Quality is the an...|     982454400|\n",
      "|B00004RALL|Patio_Lawn_and_Garde|  [0, 0]|    5.0|Just what I  expe...|05 16, 2013|A1N7X8D30UA8X7|                 mel|            Mini BBQ|    1368662400|\n",
      "+----------+--------------------+--------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the input data as DataFrame\n",
    "df = spark.read.json(\"hdfs:///user/dic23_shared/amazon-reviews/full/reviews_devset.json\")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also load the stopwords from the txt file into a list called 'stopwords'."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ec447c9-bba5-412a-9b70-86e8d6c32a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'aa', 'able', 'about', 'above', 'absorbs', 'accord', 'according', 'accordingly', 'across', 'actually', 'after', 'afterwards', 'again', 'against', 'ain', 'album', 'album', 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'app', 'appear', 'appreciate', 'appropriate', 'are', 'aren', 'around', 'as', 'aside', 'ask', 'asking', 'associated', 'at', 'available', 'away', 'awfully', 'b', 'baby', 'bb', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'believe', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'bibs', 'bike', 'book', 'books', 'both', 'brief', 'bulbs', 'but', 'by', 'c', 'came', 'camera', 'can', 'cannot', 'cant', 'car', 'case', 'cause', 'causes', 'cd', 'certain', 'certainly', 'changes', 'clearly', 'co', 'coffee', 'com', 'come', 'comes', 'concerning', 'consequently', 'consider', 'considering', 'contain', 'containing', 'contains', 'corresponding', 'could', 'couldn', 'course', 'currently', 'd', 'definitely', 'described', 'despite', 'did', 'didn', 'different', 'do', 'does', 'doesn', 'dog', 'dogs', 'doing', 'doll', 'don', 'done', 'down', 'downwards', 'during', 'e', 'each', 'edu', 'eg', 'eight', 'either', 'else', 'elsewhere', 'enough', 'entirely', 'especially', 'et', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'exactly', 'example', 'except', 'f', 'far', 'few', 'fifth', 'film', 'first', 'five', 'flavor', 'followed', 'following', 'follows', 'for', 'former', 'formerly', 'forth', 'four', 'from', 'fun', 'further', 'furthermore', 'g', 'game', 'game', 'get', 'gets', 'getting', 'given', 'gives', 'go', 'goes', 'going', 'gone', 'got', 'gotten', 'greetings', 'grill', 'guitar', 'h', 'had', 'hadn', 'hair', 'happens', 'hardly', 'has', 'hasn', 'have', 'haven', 'having', 'he', 'hello', 'help', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'hi', 'him', 'himself', 'his', 'hither', 'hopefully', 'how', 'howbeit', 'however', 'i', 'ie', 'if', 'ignored', 'immediate', 'in', 'inasmuch', 'inc', 'indeed', 'indicate', 'indicated', 'indicates', 'ink', 'inner', 'insofar', 'install', 'instead', 'into', 'inward', 'is', 'isn', 'it', 'its', 'itself', 'j', 'just', 'k', 'keep', 'keeps', 'kept', 'kitchen', 'knife', 'know', 'known', 'knows', 'l', 'lamp', 'laptop', 'last', 'lately', 'later', 'latter', 'latterly', 'least', 'less', 'lest', 'let', 'life', 'like', 'liked', 'likely', 'little', 'll', 'look', 'looking', 'looks', 'ltd', 'm', 'mainly', 'many', 'may', 'maybe', 'me', 'mean', 'meanwhile', 'merely', 'might', 'mon', 'more', 'moreover', 'most', 'mostly', 'movie', 'mower', 'much', 'must', 'my', 'myself', 'n', 'name', 'namely', 'nd', 'near', 'nearly', 'necessary', 'need', 'needs', 'neither', 'never', 'nevertheless', 'new', 'next', 'nine', 'no', 'nobody', 'non', 'none', 'noone', 'nor', 'normally', 'not', 'nothing', 'novel', 'now', 'nowhere', 'o', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'on', 'once', 'one', 'ones', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'own', 'p', 'particular', 'particularly', 'per', 'perhaps', 'phone', 'placed', 'please', 'plus', 'possible', 'presumably', 'printer', 'probably', 'product', 'provides', 'q', 'que', 'quite', 'qv', 'r', 'rather', 'rd', 're', 'read', 'read', 'really', 'reasonably', 'regarding', 'regardless', 'regards', 'relatively', 'respectively', 'right', 's', 'said', 'same', 'saw', 'say', 'saying', 'says', 'second', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', 'shave', 'she', 'shoes', 'should', 'shouldn', 'since', 'six', 'skin', 'so', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'song', 'songs', 'soon', 'sorry', 'specified', 'specify', 'specifying', 'still', 'story', 'strings', 'stroller', 'sub', 'such', 'sup', 'sure', 't', 'take', 'taken', 'taste', 'tell', 'tends', 'th', 'than', 'thank', 'thanks', 'thanx', 'that', 'that', 'thats', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'theres', 'thereupon', 'these', 'they', 'think', 'third', 'this', 'thorough', 'thoroughly', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'took', 'toward', 'towards', 'toy', 'tried', 'tries', 'truck', 'truly', 'try', 'trying', 'twice', 'two', 'u', 'un', 'under', 'unfortunately', 'unless', 'unlikely', 'until', 'unto', 'up', 'upon', 'us', 'use', 'used', 'useful', 'uses', 'using', 'usually', 'v', 'value', 'various', 've', 'very', 'via', 'viz', 'vs', 'want', 'wants', 'was', 'wasn', 'way', 'we', 'wear', 'welcome', 'well', 'went', 'were', 'weren', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'willing', 'wish', 'with', 'within', 'without', 'won', 'wonder', 'would', 'wouldn', 'x', 'y', 'yes', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves', 'z', 'zero', '']\n"
     ]
    }
   ],
   "source": [
    "# Get the path to the directory containing the Jupyter notebook\n",
    "notebook_path = os.path.abspath(\"\")\n",
    "\n",
    "# Load the stopwords into a list\n",
    "stopwords_file = open(f\"{notebook_path}/stopwords.txt\", \"r\")\n",
    "words_data = stopwords_file.read()\n",
    "stopwords = words_data.split(\"\\n\")\n",
    "stopwords_file.close()\n",
    "\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c9cee2-727e-461a-8e97-10e83698b990",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "In the following cells, we will created the different PySpark ML pipeline stages required as preparation for the classificaton task in Part 3."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Firstly, we create a RegexTokenizer object which extracts tokens form the given 'reviewText' column based on the specifications provided in the exercise description (at whitespaces, tabs, digits, and the delimiter characters ()[]{}.!?,;:+=-_\"'\\`~#@&*%€$§\\/).\n",
    "RegexTokenizer contains a parameter `toLowercase` which is set to `True` by default. It specifies to convert all characters to lowercase before tokenizing. Hence, casefolding is also taken care of with this regex based tokenizer.\n",
    "The resulting tokens after tokenization will be saved in the column 'tokens' as specified by 'outputCol'."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = RegexTokenizer(pattern=r'[\\s\\d\\(\\)\\[\\]\\{\\}\\.,!?\\-,;:+\\=_\"\\'`~#@&*%€$§\\\\/]+',\n",
    "                           inputCol=\"reviewText\",\n",
    "                           outputCol=\"tokens\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We then create a StopWordsRemover object called SWremover which is a feature transformer that filters out stop words from a given tokenized input. It removes the stopwords from stopwords.txt saved in the 'stopwords' list from the already tokenized text column.\n",
    "StopWordsRemover hence uses as input the output column of the tokenizer, specified by `tokenizer.getOutputCol()` as 'inputCol'.\n",
    "The filtered tokens will be saved in a new column 'filtered_tokens'."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SW_remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered_tokens\", stopWords=stopwords)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we create the TFIDF-stages of the pipeline, consisting of `CountVerctorizer` stage and the `IDF` stage.\n",
    "\n",
    "The `CountVectorizer` object uses the filtered tokens resulting from the StopWordsRemover (`SW_remover.getOutputCol()`) and converts them into a bag-of-words representation. The resulting features are word counts, where each feature represents the number of times a word appears in a document. The `minDF` parameter specifies the minimum number of documents in which a word must appear in order to be included in the vocabulary. It is set to its default value of 1.0.\n",
    "\n",
    "The `IDF` stage then takes the output of the CountVectorizer (`count_vectorizer.getOutputCol()`) and applies Inverse Document Frequency (IDF) weighting to those bag-of-words features. That is, it scales each feature and usually down-weights features which appear more frequently in a set of documents. The `minDocFreq` parameter specifies the minimum number of documents in which a term must appear in order to be included in the IDF calculation. It is set to its default value of 0."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(inputCol=SW_remover.getOutputCol(), outputCol=\"cv_features\", minDF=1.0) #vocabSize=2048,\n",
    "idf = IDF(inputCol=count_vectorizer.getOutputCol(), outputCol=\"tfidf_features\", minDocFreq = 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the final stage, we create a `ChiSqSelector` object called chi_sq_selector to select the top features, here tokens, based on a chi-squared statistical test between each token and the target label for the subsequent classification task, here the different product categories.\n",
    "\n",
    "As `ChiSqSelector` requires numeric labels as inputs, we first use `StringIndexer` to map our string 'category' column to a numerical index which is saved in the column 'label'.\n",
    "\n",
    "The chi_sq_selector then takes as input the tokens, i.e. the output of the idf stage (`idf.getOuputCol()`), as well as the labels contained in the 'label' column and selects the top 2000 features (across all classes) based on the a chi-squared statistical test between each feature and the label. The selected features are stored in a new column called 'selected_features'."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cat_Indexer = StringIndexer(inputCol=\"category\", outputCol=\"label\")\n",
    "chi_sq_selector = ChiSqSelector(numTopFeatures=2000, featuresCol=idf.getOutputCol(), outputCol=\"selected_features\", labelCol=cat_Indexer.getOutputCol())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lastly, we combine the above defined stages into a single pipeline, with the order of the stages resembling the order of the cells above."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d33b2c97-0bb2-44c8-a372-5124d6ca6389",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer, SW_remover, count_vectorizer, idf, cat_Indexer, chi_sq_selector])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that the pipeline is defined, we can train the pipeline model on the given data stored in the DataFrame df."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train the pipeline model\n",
    "pipeline_model = pipeline.fit(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to extract the top 2000 terms selected by the `ChiSqSelector` we define the following function `save_selected_terms()`. It takes a pipeline model and an output txt file and writes the terms to the file in the following steps:\n",
    "\n",
    "1. Extract the CountVectorizer and ChiSqSelector models from the pipeline model using the stages attribute and indexing.\n",
    "2. From the CountVectorizerModel, extract the list of terms in the same order as the corresponding indeces in the feature vectors using the model's `vocabulary` attribute.\n",
    "3. Obtain a list of indices of the 2000 selected features using the `selectedFeatures` attribute of the ChiSqSelectorModel.\n",
    "4. Create the list of selected terms from the selected indices and the corresponding term in the vocabulary list and sort it alphabetically.\n",
    "5. Write the list of selected terms to the output file, separated by commas."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46ce1cde-76c7-4d67-b5c8-7389950fdced",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/05 10:47:20 WARN DAGScheduler: Broadcasting large task binary with size 1233.6 KiB\n",
      "23/05/05 10:47:20 WARN DAGScheduler: Broadcasting large task binary with size 1235.7 KiB\n",
      "23/05/05 10:47:24 WARN DAGScheduler: Broadcasting large task binary with size 1238.7 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def save_selected_terms(pipeline_model, output_file):\n",
    "    count_vectorizer_model = pipeline_model.stages[2]\n",
    "    chi_sq_selector_model = pipeline_model.stages[-1]\n",
    "\n",
    "    #Extract the list of terms\n",
    "    vocabulary = count_vectorizer_model.vocabulary\n",
    "    #Obtain the list of indeces\n",
    "    selected_indices = chi_sq_selector_model.selectedFeatures\n",
    "\n",
    "    # Create a list of selected terms\n",
    "    selected_terms = [vocabulary[index] for index in selected_indices]\n",
    "    selected_terms.sort()\n",
    "    print('# terms selected:', len(selected_terms))\n",
    "\n",
    "    # Write the list to the output file\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for term in selected_terms:\n",
    "            f.write(f\"{term} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this last cell the results are written to a file called \"output_ds.txt\" using the above function. As specified in the exercise description, it contains the top 2000 terms across all sentiment classes (in our case all product categories) in alphabetical order."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_file = \"output_ds.txt\"\n",
    "save_selected_terms(pipeline_model, output_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb6d7a51-828f-49aa-a3d5-31326a8cad8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DIC23)",
   "language": "python",
   "name": "python3_dic23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
