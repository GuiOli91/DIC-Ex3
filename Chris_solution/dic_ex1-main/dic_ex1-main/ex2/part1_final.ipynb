{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee613624",
   "metadata": {},
   "source": [
    "This is the Jupyter Notebook for Part 1 of exercise 2\n",
    "\n",
    "In the first code cell we simply import necessary libraries and set up the Spark application using \n",
    "SparkConf and SparkContext from the pyspark library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86553e9-0588-4e21-b25c-958acf32e4f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "23/04/27 09:54:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# Configure the Spark application and create a SparkContext\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"ChiSquareJob\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3b9e35c",
   "metadata": {},
   "source": [
    "Next we load the input data from the given HDFS path in an RDD named input_data. \n",
    "We also load the stopwords from the txt file in the same directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b5b0161-7d92-4fe1-9768-ebe6d802ea55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get the path to the directory containing the Jupyter notebook\n",
    "notebook_path = os.path.abspath(\"\")\n",
    "\n",
    "# Load the input data and stopwords\n",
    "input_data = sc.textFile(\"hdfs:///user/dic23_shared/amazon-reviews/full/reviews_devset.json\")\n",
    "stopwords = set(sc.textFile(f\"file://{notebook_path}/stopwords.txt\").collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0fcf9497",
   "metadata": {},
   "source": [
    "In the following cell we implement a function which tokenizes the reviews. It applies the transformations which were requested in exercise 1 (e.g. lowercase, excluding stopwords and one-letter words). It also uses regex to split the text and filter it based on the given criteria. For computing the chi squared value we are only interested in the unique_tokens of the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85bb3eba-6f5c-48ed-b7e9-34b24b1f8ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(review):\n",
    "    tokenizer = re.compile(r'[\\s\\d\\(\\)\\[\\]\\{\\}\\.,!?\\-,;:+\\=_\"\\'`~#@&*%€$§\\\\/]+')\n",
    "    tokens = tokenizer.split(review)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    unique_tokens = set(token for token in tokens if token not in stopwords and len(token) > 1)\n",
    "    return unique_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f40cce9",
   "metadata": {},
   "source": [
    "Next we implement a calculate_chi_squared function based on the given formula from the lecture. \n",
    "The function returns a dictionary containing the chi-squared values for the provided token in each category.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6b6270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chi_squared(token, counts, total_word_documents):\n",
    "    chi_squared_values = {}\n",
    "    for category, count in counts.items():\n",
    "        A = count\n",
    "        B = total_word_documents - A\n",
    "        C = category_counts[category] - A\n",
    "        D = N - A - B - C\n",
    "        upper_part = N * (A * D - B * C) ** 2\n",
    "        lower_part = (A + B) * (A + C) * (B + D) * (C + D)\n",
    "        chi_squared = upper_part / lower_part\n",
    "        chi_squared_values[category] = chi_squared\n",
    "    return chi_squared_values\n",
    "\n",
    "# A - number of documents in c which contain t\n",
    "# B - number of documents not in c which contain t\n",
    "# C - number of documents in c without t\n",
    "# D - number of documents not in c without t"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19c31416",
   "metadata": {},
   "source": [
    "In the following code snippet we compute the category counts from the input data. First, the data is loaded as JSON. Next the categories are extracted, and then the categories are reduced to their counts. The result is an RDD containing the category counts, which is then converted to a dictionary. The total number of documents (N) is calculated as the sum of the category counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "022195e4-58e3-4446-ae7d-e10fba5f57fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Compute the category counts from the input data\n",
    "category_counts_rdd = input_data.map(json.loads).map(lambda review: (review['category'], 1)).reduceByKey(lambda a, b: a + b)\n",
    "category_counts = dict(category_counts_rdd.collect())\n",
    "N = sum(category_counts.values())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "446eee2d",
   "metadata": {},
   "source": [
    "In the following code we perform several transformations and actions on our input_data RDD. Overall this pipeline computes for each token-category pair the chi-squared value. Below we describe the steps. \n",
    "\n",
    "1. Using `map(json.loads)` we Parse the input lines as a JSON object into a python directory.\n",
    "2. In `flatMap(...)` the review text is tokenized. Stopwords and words with length 1 are excluded. The ouput has the format `((token, category), 1)`.\n",
    "3. We sum the counts of each `(token, category)` pair by calling `reduceByKey(...)`.\n",
    "4. Using `map(...)` we rearrange the tuples to the format `(token, (category, count))`.\n",
    "5. `groupByKey()`: Group the token counts by token, resulting in `(token, [(category_1, count_1), ...])`. This is needed for our chi_squared formula. \n",
    "6. The next `map(...)` function converts the list of category and count pairs to a dictionary.\n",
    "7. Now we calculate the chi-squared values for each token and category pair using the `calculate_chi_squared` function in `map(...)`.\n",
    "8. To get the format `((token, category), chi_squared)` we flatten the chi-squared values into tuples using `flatMap(...)`\n",
    "\n",
    "Through this pipeline we obtain an RDD, `chi_squared_values` for each token and category pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f21d1b1b-cb6a-4105-bc2d-6ff69c2630db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform the transformations and actions on RDDs\n",
    "chi_squared_values = input_data \\\n",
    "    .map(json.loads) \\\n",
    "    .flatMap(lambda review: [((token, review['category']), 1) for token in tokenize(review['reviewText']) if token not in stopwords and len(token) > 1]) \\\n",
    "    .reduceByKey(lambda x, y: x + y) \\\n",
    "    .map(lambda x: (x[0][0], (x[0][1], x[1]))) \\\n",
    "    .groupByKey() \\\n",
    "    .map(lambda token_counts: (token_counts[0], {category: count for category, count in token_counts[1]})) \\\n",
    "    .map(lambda token_counts: (token_counts[0], calculate_chi_squared(token_counts[0], token_counts[1], sum(token_counts[1].values())))) \\\n",
    "    .flatMap(lambda token_chi_squared: [((token_chi_squared[0], category), chi_squared) for category, chi_squared in token_chi_squared[1].items()])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eae855cf",
   "metadata": {},
   "source": [
    "The following pipeline takes the `chi_squared_values` RDD, which we computed in the previous cell and mereges the chi-quared values for the same cateogry and token. Lastly, it extracts the to 75 terms. \n",
    "\n",
    "1.  We rearrange the tuples to the format `(category, (token, chi_squared))` using the `map(...)` function. \n",
    "2.  Next, we group the chi-squared values by category using `groupByKey()`. This results in `(category, [(token_1, chi_squared_1), ...])`.\n",
    "3.  Lastly, `mapValues(...)` is used to sort the list of token and chi-squared pairs in descending order based on the chi-squared value. Here, we only keep the top 75 most discriminative terms for each category.\n",
    "\n",
    "This pipeline leads to an RDD which contains the top 75 terms for each category with their chi-squared values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bb71323-3b2e-4014-8e6b-08cbb4a8683d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge the chi-squared values for the same category and token\n",
    "merged_chi_squared = chi_squared_values \\\n",
    "    .map(lambda x: (x[0][1], (x[0][0], x[1]))) \\\n",
    "    .groupByKey() \\\n",
    "    .mapValues(lambda chi_squared_values: sorted(chi_squared_values, key=lambda x: x[1], reverse=True)[:75])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc5b37b2",
   "metadata": {},
   "source": [
    "In the last cell the results are written a file named \"results_part1.txt\". As specified in the exercise the cateogry name is followed by the top 75 terms with their chi-squared values in descending order, one category per line. Finally, the merged directory is written. It contains all unique terms across all categories in alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e371a85-11de-4dbc-87ae-e93f2c4f47cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"results_part1.txt\", \"w\") as f:\n",
    "    top_terms_per_category = sorted(merged_chi_squared.collect(), key=lambda x: x[0])\n",
    "    merged_dictionary = sorted(merged_chi_squared.flatMap(lambda kv: [term for term, _ in kv[1]]).distinct().collect())\n",
    "\n",
    "    for category, top_terms in top_terms_per_category:\n",
    "        f.write(f\"{category} {' '.join(f'{term}:{chi_squared:.4f}' for term, chi_squared in top_terms)}\\n\")\n",
    "    f.write(' '.join(merged_dictionary))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DIC23)",
   "language": "python",
   "name": "python3_dic23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
